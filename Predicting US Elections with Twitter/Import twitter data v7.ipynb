{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 Predicting elections using twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOALS\n",
    "\n",
    "1. Analyse the social media posts (i.e., tweets) related to the upcoming elections in the USA. Example questions that you should aim to answer include, but are not limited to:\n",
    "\n",
    "- What is the sentiment towards the candidates? This will involve extracting sentiment from the textual content of the tweets, using one of the toolkits mentioned in the lectures, such as SentiStrength.\n",
    "\n",
    "- Where are they more popular? The tweets can be associated with a particular geolocation, which can be further used to study the geographic distribution of political preferences.\n",
    "\n",
    "- Which topics are they and their supporters talking about? In the second week of the use case we will talk in more detail about the topical analysis. Until then, you could start by e.g., analysing the hashtags associated with each of the candidates.\n",
    "\n",
    "- How big is the divide between the supporters? Here you could, for example, analyse whether the users normally follow and interact with their preferred candidate and like-minded supporters or they are interacting with the opponent's social network as well.\n",
    "\n",
    "2. Investigate the existence of a relation between voting preferences and various statistics, related to e.g. demographics, education, income, health care and religion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in order to execute in kike's computer... \n",
    "# if missing any submodule from nltk, run in python: nltk.download()\n",
    "\n",
    "import math\n",
    "from shapely.geometry import Point, mapping, shape\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from shapely.geometry import Point, mapping, shape\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import random\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib.colors import rgb2hex\n",
    "from descartes import PolygonPatch\n",
    "from shapely.geometry import Polygon, MultiPolygon, shape\n",
    "import shapely.affinity # <- test this for scaling\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import csv\n",
    "from  matplotlib.pyplot import colorbar\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob\n",
    "# prepare for Python version 3x features and functions\n",
    "from __future__ import division, print_function\n",
    "# import packages for text processing and multivariate analysis\n",
    "import re  # regular expressions\n",
    "import nltk  # draw on the Python natural language toolkit\n",
    "import pandas as pd  # DataFrame structure and operations\n",
    "import numpy as np  # arrays and numerical processing\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt  # 2D plotting\n",
    "# terms-by-documents matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans  # cluster analysis by partitioning\n",
    "from sklearn.decomposition import PCA  # principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all tweets to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steps to use MongoDB:\n",
    "# 1. pip install pymongo\n",
    "# 2. install MongoDB for windows\n",
    "# 3. create folder C:\\data\\db\n",
    "# 4. C:\\Program Files\\MongoDB\\Server\\3.2\\bin>mongoimport.exe /collection:election_tweets \n",
    "        # D:\\Master\\FDS\\works\\work2\\data\\geotagged_tweets_20160812-0912.jsons\n",
    "# 5. C:\\Program Files\\MongoDB\\Server\\3.2\\bin>mongod.exe\n",
    "\n",
    "client = MongoClient()\n",
    "#client.server_info()  # test purpose\n",
    "db = client.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that we actually have a connection to the MongoDB\n",
    "db.election_tweets[1]\n",
    "for a in db.get_collection('election_tweets').find({}).limit(10):\n",
    "    for b in a[\"entities\"][\"hashtags\"]:\n",
    "        print(b[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open list of 1000 common english words. Source Facebook uploaded by David Langerveld\n",
    "DIR = \"./words.txt\"\n",
    "with open(DIR) as f:\n",
    "    common_words = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define list of codes to be dropped from documents\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']    \n",
    "\n",
    "# contractions and other word strings to drop from further analysis, adding\n",
    "# to the usual English stopwords to be dropped from the document collection\n",
    "more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "    'shes','thats','theres','theyre','wont','youll','youre','youve',\\\n",
    "    're','tv','g','us','en','ve','vg','didn','pg','gp','our','we',\n",
    "    'll','film','video','name','years','days','one','two','three', 'amp',\\\n",
    "    'four','five','six','seven','eight','nine','ten','eleven','twelve'] \n",
    "# start with the initial list and add to it for movie text work \n",
    "# nltk.download('corpus')\n",
    "stoplist = nltk.corpus.stopwords.words('english') + more_stop_words + common_words.split()\n",
    "\n",
    "# maybe using BeautifulSoup\n",
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    # replace selected character strings/stop-words with space\n",
    "    for i in range(len(stoplist)):\n",
    "        stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "        temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    \n",
    "\n",
    "def RemoveHTMLTags(data):\n",
    "    p = re.compile(r'<[^<]*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def RemoveLinks(data):\n",
    "    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"\", data)\n",
    "\n",
    "def extractLinks(data):\n",
    "    p = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return p.findall(data)\n",
    "\n",
    "def removeMentions(data):\n",
    "    return re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))@([A-Za-z0-9_]+)', \"\", data)\n",
    "\n",
    "def removeHashtags(data):\n",
    "    return re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))#([A-Za-z0-9_]+)', \"\", data)\n",
    "\n",
    "def extractWebText(link):\n",
    "    response = urllib2.urlopen(link)\n",
    "    html = response.read()\n",
    "    html = re.sub(r'\\n\\s*\\n', '\\r\\n', html)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    for elem in soup.find_all(['script', 'style']):\n",
    "        elem.extract()\n",
    "\n",
    "    return text_parse(RemoveHTMLTags(removeMentions(RemoveLinks(soup.get_text()))))\n",
    "\n",
    "def stemText(clean_text):\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    return \" \".join([str(stemmer.stem(word)) for word in clean_text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating a index \n",
    "db.election_tweets.create_index(\"place.country_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to segment in groups \n",
    "# we split in 3 groups: Hillary, Trump, Neutral\n",
    "# remember that _id is the mongodb id and id is twitter's id\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "hillary = []\n",
    "trump = []\n",
    "neutral = []\n",
    "tweets_count = 0\n",
    "# defining hashtags that define groups\n",
    "hillary_tags = ['nevertrump', 'imwithher', 'dumptrump','drumpf','trumptrainwreck', 'clintonfoundation', 'voteblue', 'hillary4us', 'notjustanywomen', 'trumpsodumb', 'hillnotjill']\n",
    "trump_tags = ['crookedhillary','neverhillary', 'trumppence16','maga', \"hillno\", 'imnotwithher', 'clintonnewsnetwork', 'trumptrain', 'makeamericagreatagain', 'latinosfortrump', 'lockherup', 'gaysfortrump', 'trumpforpresident']\n",
    "neutral_tags = ['donaldtrump','hillaryclinton', 'trump', 'trump', 'hillary', 'realdonaldtrump', 'jillnothill', 'hillaryclinton', 'election2016','jillstein', 'gogreen','greenparty','trump2016']\n",
    "\n",
    "coll = db.election_tweets\n",
    "#bulkop = coll.initialize_unordered_bulk_op()\n",
    "\n",
    "\n",
    "# Using hashtags to split\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "            {\"place.country_code\":'US'},\n",
    "            { \"clean\" : { '$exists':False }}\n",
    "        ]}).batch_size(100):\n",
    "    h = 0\n",
    "    t = 0\n",
    "    n = 0\n",
    "    tweets_count += 1\n",
    "    if (tweets_count % 50000 == 0):\n",
    "        print(\"%i tweets processed in %i seconds...\" % (tweets_count, time.time() - t0))\n",
    "    links = extractLinks(a[\"text\"])\n",
    "    \n",
    "    # Extract and add the links to the document\n",
    "    if links:\n",
    "        retval = db.election_tweets.update_one({'id':a[\"id\"]},{'$addToSet':{'links': {'$each': links} }})\n",
    "    \n",
    "    # Stemming\n",
    "    #stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    #print(stemmer.stem(\"wordings\"))\n",
    "    \n",
    "    \n",
    "    clean_text = text_parse(RemoveHTMLTags(RemoveLinks(removeMentions(removeHashtags(a[\"text\"])))))\n",
    "    sentiment_text = TextBlob(clean_text).sentiment\n",
    "    polarity = sentiment_text.polarity\n",
    "    subjectivity = sentiment_text.subjectivity\n",
    "    \n",
    "    # Add clean text to the document\n",
    "    if a[\"text\"]:\n",
    "        retval = db.election_tweets.update_one({'id':a[\"id\"]}, {'$set':{'clean_text':clean_text,\n",
    "                                                                        'subjectivity':subjectivity,\n",
    "                                                                        'polarity': polarity,\n",
    "                                                                        'clean' : True}}) \n",
    "    \n",
    "    # Find out the group to which the tweet belongs to and add this information to the document\n",
    "    tag = a[\"entities\"][\"hashtags\"]\n",
    "    for b in tag:\n",
    "        text = b[\"text\"]\n",
    "        text = text.lower()\n",
    "        if text in hillary_tags:\n",
    "            h += 1\n",
    "        elif text in trump_tags:\n",
    "            t += 1\n",
    "        elif text in neutral_tags:\n",
    "            n += 1\n",
    "    if h > t and h > n:\n",
    "        hillary.append(a[\"id\"])\n",
    "        retval = db.election_tweets.update_one({'id':a[\"id\"]},\n",
    "        {\n",
    "            '$set':{\n",
    "                'group':\"hillary\"\n",
    "                    }\n",
    "        }\n",
    "        )\n",
    "    elif t > h and t > n:\n",
    "        trump.append(a[\"id\"])\n",
    "        retval = db.election_tweets.update_one({'id':a[\"id\"]},\n",
    "        {\n",
    "            '$set':{\n",
    "                'group':\"trump\"\n",
    "                    }\n",
    "        }\n",
    "        )\n",
    "    elif n > t and n > h:\n",
    "        neutral.append(a[\"id\"])\n",
    "        retval = db.election_tweets.update_one({'id':a[\"id\"]},\n",
    "        {\n",
    "            '$set':{\n",
    "                'group':\"neutral\"\n",
    "                    }\n",
    "        }\n",
    "        )\n",
    "    else:\n",
    "        neutral.append(a[\"id\"])\n",
    "        retval = db.election_tweets.update_one({'id':a[\"id\"]},\n",
    "        {\n",
    "            '$set':{\n",
    "                'group':\"neutral\"\n",
    "                    }\n",
    "        }\n",
    "        )\n",
    "\n",
    "# Write operation in MongoDB\n",
    "print(\"writing...\")\n",
    "#retval = bulkop.execute()\n",
    "print(\"done!\")\n",
    "print(retval)\n",
    "print(\"From %i tweets, %i are pro-hillary, %i are pro-trump, %i are neutral\" % (tweets_count, len(hillary), len(trump), len(neutral)))\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(\"%i total seconds\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment toolkits comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_tweets = len(tweets)\n",
    "target = open(\"cleaned_tweets.txt\", 'w')\n",
    "cleaned_words = []\n",
    "\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "            {\"place.country_code\":'US'},\n",
    "            { \"clean\" : { '$exists':False }}\n",
    "        ]}).limit(10000):\n",
    "    cleaned_tweet = text_parse(RemoveHTMLTags(RemoveLinks(tweets[i][\"text\"])))\n",
    "    cleaned_words.extend(cleaned_tweet.split())\n",
    "    \n",
    "    cleaned_tweet = text_parse(RemoveHTMLTags(RemoveLinks(tweets[i][\"text\"])))\n",
    "    #print(cleaned_tweet)\n",
    "    target.write(str(i))\n",
    "    target.write('\\t')\n",
    "    target.write(a[\"clean_text\"].strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\"))\n",
    "    target.write('\\n')\n",
    "    textblob_polarity = a[\"polarity\"]\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(cleaned_tweet)\n",
    "    vader_polarity = ss[\"compound\"]\n",
    "\n",
    "#    for k in sorted(ss):\n",
    "#        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "#        print()\n",
    "\n",
    "    print(\"%i\\tTextBlob polarity score\\t%f\\tVader polarity score\\t%f\" % (int(i+1), float(textblob_polarity), float(vader_polarity)))\n",
    "# in order to compare the results also with SentiStrength, we need to execute this:\n",
    "# D:\\Master\\FDS\\works\\work2\\sentistrength\\sentistrength>java -jar \n",
    "#   SentiStrengthCom.jar sentidata D:/Master/FDS/works/work2/sentistrength/sentistrength/SentStrength_Data_December2015English/ \n",
    "#   input D:/Master/FDS/works/work2/cleaned_tweets.txt scale\n",
    "\n",
    "# Then, we should append both the data from Vader and TextBlob with the one outputted by SentiStrength and do some statistics in\n",
    "#   excel (tab separated data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting number of tweets vs timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "days = []\n",
    "for tweet in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "            { \"place.country_code\":'US' },\n",
    "            { \"group\" : 'trump' }\n",
    "            #}\n",
    "        ]}):\n",
    "    date = datetime.datetime.fromtimestamp(float(unicode(tweet[\"timestamp_ms\"])) / 1e3)\n",
    "    days.append(date.day)\n",
    "plt.figure()\n",
    "plt.hist(days, bins= 31,color=\"green\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of tweets in database')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a map with majority of groups of tweets with each color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first are going to do the inverse lookup of the coordinates and gather the name of the State to which the tweet belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "coordinates = []\n",
    "x = 0\n",
    "for tweet in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "            { \"place.country_code\":'US' },\n",
    "            { \"group\" : {'$exists':'true'}},\n",
    "            { \"polarity\" : {'$exists':'true'}}\n",
    "        ]}):\n",
    "    if tweet['place']['bounding_box']['coordinates'][0][0]:\n",
    "        coordinates.append([tweet['place']['bounding_box']['coordinates'][0][0][0],\n",
    "                            tweet['place']['bounding_box']['coordinates'][0][0][1],\n",
    "                            tweet['group'],\n",
    "                            tweet['polarity'],\n",
    "                            tweet['subjectivity']])\n",
    "                            \n",
    "\n",
    "print(\"%d coordinates found\" % len(coordinates) )\n",
    "print(coordinates[:22], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BLUE = '#5599ff'\n",
    "RED = '#ff0000'\n",
    "title = \"Balance of groups per state\"\n",
    "\n",
    "MAPS_DIR = '.'\n",
    "with open(os.path.join(MAPS_DIR, 'states.geojson')) as rf:\n",
    "    maps = json.load(rf)\n",
    "\n",
    "state_score = {}\n",
    "for feature in maps['features']:\n",
    "    state_name = feature['properties'][u'STATE_NAME'] # state name\n",
    "    polygon = shape(feature['geometry']) \n",
    "    try:\n",
    "        if not state_score[state_name]:\n",
    "            state_score[state_name] = [0, 0, 0, 0] # state score, total polarity, total subjectivity, total tweets\n",
    "    except KeyError, e:\n",
    "        state_score[state_name] = [0, 0, 0, 0]\n",
    "    for point in coordinates:\n",
    "        point_ = Point(point[0], point[1])\n",
    "        if polygon.contains(point_):\n",
    "            state_score[state_name][1] += point[3]\n",
    "            state_score[state_name][2] += point[4]\n",
    "            state_score[state_name][3] += 1\n",
    "            if point[2] == \"hillary\":\n",
    "                state_score[state_name][0] += -(0.5 + 0.5 * math.fabs(point[3]))\n",
    "                #print(\"hill:\", -(0.5 + 0.5 * math.fabs(point[3])))\n",
    "            elif point[2] == \"trump\":\n",
    "                state_score[state_name][0] += 0.5 + 0.5 * math.fabs(point[3])\n",
    "                #print(\"trump:\", 0.5 + 0.5 * math.fabs(point[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_state_score = 0\n",
    "max_state_score = 0\n",
    "for score in state_score.itervalues():\n",
    "    if score[0] > 0:\n",
    "        score[0] = math.log(1 + score[0])\n",
    "    elif score[0] < 0:\n",
    "        score[0] = -math.log(1 - score[0])\n",
    "    if score[0] < min_state_score:\n",
    "        min_state_score = score[0]\n",
    "    elif score[0] > max_state_score:\n",
    "        max_state_score = score[0]\n",
    "        \n",
    "print(\"min score is %f and max score is %f\" % (min_state_score, max_state_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(range(len(state_score)), [sc[0] for sc in state_score.itervalues()], align='center',color=\"white\")\n",
    "plt.grid()\n",
    "plt.xticks(range(len(state_score)), state_score.keys())\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.gca()\n",
    "\n",
    "cm_ = plt.get_cmap('seismic') # Name of colormap we want to use\n",
    "# Careful with normalizing, it will not automatically calculate percentages, it will only paint absolute values\n",
    "cNorm  = colors.Normalize(vmin=min_state_score, vmax=max_state_score) # Range of values expected in order to be able to normalize any incoming data\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm_) # Create the scalarMap itself\n",
    "\n",
    "for feature in maps['features']:\n",
    "    state_name = feature['properties'][u'STATE_NAME'] # state name\n",
    "    state_fips = feature['properties'][u'STATE_FIPS'] # example of data to be used\n",
    "    geometry = feature['geometry']\n",
    "    s = shape(geometry)\n",
    "    #colorVal = scalarMap.to_rgba(values[state_name])\n",
    "    colorVal = scalarMap.to_rgba(state_score[state_name][0])\n",
    "\n",
    "    if state_name == 'Hawaii': # make it bigger and move it to the right\n",
    "        s = shapely.affinity.scale(s,xfact=3, yfact=3)\n",
    "        s = shapely.affinity.translate(s, xoff=35, yoff=-6)\n",
    "    elif state_name == 'Alaska': # make it a little bit smaller and move it down\n",
    "        s = shapely.affinity.scale(s,xfact=0.5, yfact=0.5 )\n",
    "        s = shapely.affinity.translate(s, xoff =28 , yoff=-35) \n",
    "    # print(state_name + \" - \" + total_hospitals[state_name])\n",
    "    if geometry['type'] == 'Polygon':\n",
    "        ax.add_patch(PolygonPatch(s, fc=colorVal,ec=BLUE, alpha=0.5, zorder=2))\n",
    "    else:\n",
    "        for g in s.geoms:\n",
    "            ax.add_patch(PolygonPatch(g,fc=colorVal, ec=BLUE, alpha=0.5, zorder=2))\n",
    "    plt.text(s.centroid.x, s.centroid.y,state_name, fontsize=6)\n",
    "\n",
    "ax.axis('scaled')\n",
    "plt.axis('off')\n",
    "plt.title(title)\n",
    "cmmapable = cmx.ScalarMappable(cNorm, cm_)\n",
    "cmmapable.set_array(range(int(min_state_score),int(max_state_score)))\n",
    "plt.colorbar(cmmapable, fraction=0.01, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cleaned_words_split_hillary = []\n",
    "cleaned_words_split_trump = []\n",
    "cleaned_words_split_neutral = []\n",
    "\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "        { \"place.country_code\":'US' },\n",
    "        { \"clean_text\" : { '$exists':'true' }},\n",
    "        { \"group\" : { '$exists':'true' }\n",
    "    }]}):\n",
    "    if a and a[\"group\"]==\"hillary\":\n",
    "        cleaned_words_split_hillary.extend(a[\"clean_text\"].split())\n",
    "    elif a and a[\"group\"]==\"trump\":\n",
    "        cleaned_words_split_trump.extend(a[\"clean_text\"].split())\n",
    "    elif a and a[\"group\"]==\"neutral\":\n",
    "        cleaned_words_split_neutral.extend(a[\"clean_text\"].split())\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Hillary 1st 50 words: \")\n",
    "print(cleaned_words_split_hillary[:50])\n",
    "\n",
    "print(\"Trump 1st 50 words: \")\n",
    "print(cleaned_words_split_trump[:50])\n",
    "\n",
    "print(\"Neutral 1st 50 words: \")\n",
    "print(cleaned_words_split_neutral[:50])\n",
    "\n",
    "vectorizer_hillary = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)\n",
    "vectorizer_trump = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)\n",
    "vectorizer_neutral = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)\n",
    "data_features_hillary = vectorizer_hillary.fit_transform(cleaned_words_split_hillary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_features_trump = vectorizer_trump.fit_transform(cleaned_words_split_trump).toarray()\n",
    "data_features_neutral = vectorizer_neutral.fit_transform(cleaned_words_split_neutral).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_features_hillary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_features_hillary = data_features_hillary.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_hillary = vectorizer_hillary.get_feature_names()\n",
    "vocab_trump = vectorizer_trump.get_feature_names()\n",
    "vocab_neutral = vectorizer_neutral.get_feature_names()\n",
    "print(vocab_hillary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features_hillary, axis=0)\n",
    "vocabulary_count_hillary = []\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab_hillary, dist):\n",
    "    vocabulary_count_hillary.append((count, tag))\n",
    "\n",
    "vocabulary_count_hillary.sort(key = lambda tup: -tup[0])\n",
    "#vocabulary_count.sort(key=lambda tup: tup[1])\n",
    "print(str(vocabulary_count_hillary[0:50]))\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features_trump, axis=0)\n",
    "vocabulary_count_trump = []\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab_trump, dist):\n",
    "    vocabulary_count_trump.append((count, tag))\n",
    "\n",
    "vocabulary_count_trump.sort(key = lambda tup: -tup[0])\n",
    "#vocabulary_count.sort(key=lambda tup: tup[1])\n",
    "print(\"\\n\" )\n",
    "print(str(vocabulary_count_trump[0:50]))\n",
    "\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features_neutral, axis=0)\n",
    "vocabulary_count_neutral = []\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab_neutral, dist):\n",
    "    vocabulary_count_neutral.append((count, tag))\n",
    "\n",
    "vocabulary_count_neutral.sort(key = lambda tup: -tup[0])\n",
    "#vocabulary_count.sort(key=lambda tup: tup[1])\n",
    "print(\"\\n\" )\n",
    "print(str(vocabulary_count_neutral[0:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\" \".join(cleaned_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kike's try\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Read the whole text.\n",
    "text = \" \".join(cleaned_words)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(height=600, width=800).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# take relative word frequencies into account, lower max_font_size\n",
    "#'''\n",
    "wordcloud = WordCloud(height=800, width=1100,max_font_size=70, relative_scaling=.3,background_color=\"white\").generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### term document matrix vectorization + euclidean distance + MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text_split_hillary = []\n",
    "cleaned_text_split_trump = []\n",
    "cleaned_text_split_neutral = []\n",
    "\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "        { \"place.country_code\":'US' },\n",
    "        { \"clean_text\" : { '$exists':'true' }},\n",
    "        { \"group\" : { '$exists':'true' }\n",
    "    }]}):\n",
    "    if a and a[\"group\"]==\"hillary\":\n",
    "        cleaned_text_split_hillary.extend(a[\"clean_text\"].split())\n",
    "    elif a and a[\"group\"]==\"trump\":\n",
    "        cleaned_text_split_trump.extend(a[\"clean_text\"].split())\n",
    "    elif a and a[\"group\"]==\"neutral\":\n",
    "        cleaned_text_split_neutral.extend(a[\"clean_text\"].split())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdm_method_hillary = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "tdm_method_trump = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "tdm_method_neutral = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "\n",
    "examine_tdm_hillary = tdm_method_hillary.fit(cleaned_text_split_hillary)\n",
    "examine_tdm_trump = tdm_method_trump.fit(cleaned_text_split_trump)\n",
    "examine_tdm_neutral = tdm_method_neutral.fit(cleaned_text_split_neutral)\n",
    "\n",
    "top_words_hillary = examine_tdm_hillary.get_feature_names()\n",
    "top_words_trump = examine_tdm_trump.get_feature_names()\n",
    "top_words_neutral = examine_tdm_neutral.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import euclidean_distances \n",
    "from sklearn.metrics.pairwise import linear_kernel as cosine_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances as manhattan_distances\n",
    "\n",
    "from sklearn import manifold  # multidimensional scaling\n",
    "# get clean printing of the top words \n",
    "#print(map(lambda t: t.encode('ascii'), top_words))  # print sans unicode\n",
    "\n",
    "# extract the terms-by-documents matrix \n",
    "# in scipy compressed sparse column format\n",
    "sparse_tdm_hillary = tdm_method_hillary.fit_transform(cleaned_text_split_hillary)\n",
    "sparse_tdm_trump = tdm_method_trump.fit_transform(cleaned_text_split_trump)\n",
    "sparse_tdm_neutral = tdm_method_neutral.fit_transform(cleaned_text_split_neutral)\n",
    "# convert sparse matrix into regular terms-by-documents matrix\n",
    "tweet_tdm_hillary = sparse_tdm_hillary.todense()\n",
    "tweet_tdm_trump = sparse_tdm_trump.todense()\n",
    "tweet_tdm_neutral = sparse_tdm_neutral.todense()\n",
    "# define the documents-by-terms matrix \n",
    "tweet_dtm_hillary = tweet_tdm_hillary.transpose()\n",
    "tweet_dtm_trump = tweet_tdm_trump.transpose()\n",
    "tweet_dtm_neutral = tweet_tdm_neutral.transpose()\n",
    "\n",
    "# movies_distance_matrix = euclidean_distances(tweet_dtm)\n",
    "# movies_distance_matrix = manhattan_distances(tweet_dtm)\n",
    "# movies_distance_matrix = cosine_distances(tweet_dtm)\n",
    "# for some reason, manhattan_distances yields all-zeros\n",
    "tweet_distance_matrix_hillary = manhattan_distances(tweet_dtm_hillary)\n",
    "tweet_distance_matrix_trump = manhattan_distances(tweet_dtm_trump)\n",
    "tweet_distance_matrix_neutral = manhattan_distances(tweet_dtm_neutral)\n",
    "\n",
    "# multidimensional scaling\n",
    "mds_method_hillary = manifold.MDS(n_components = 2, random_state = 9999,\\\n",
    "    dissimilarity = 'precomputed')\n",
    "mds_fit_hillary = mds_method_hillary.fit(tweet_distance_matrix_hillary)  \n",
    "mds_coordinates_hillary = mds_method_hillary.fit_transform(tweet_distance_matrix_hillary) \n",
    "\n",
    "mds_method_trump = manifold.MDS(n_components = 2, random_state = 9999,\\\n",
    "    dissimilarity = 'precomputed')\n",
    "mds_fit_trump = mds_method_trump.fit(tweet_distance_matrix_trump)  \n",
    "mds_coordinates_trump = mds_method_trump.fit_transform(tweet_distance_matrix_trump) \n",
    "\n",
    "mds_method_neutral = manifold.MDS(n_components = 2, random_state = 9999,\\\n",
    "    dissimilarity = 'precomputed')\n",
    "mds_fit_neutral = mds_method_neutral.fit(tweet_distance_matrix_neutral)  \n",
    "mds_coordinates_neutral = mds_method_neutral.fit_transform(tweet_distance_matrix_neutral) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot of the distances between words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(mds_coordinates_hillary[:,0],mds_coordinates_hillary[:,1],\\\n",
    "    facecolors = 'none', edgecolors = 'none')  # plots points in white (invisible)\n",
    "labels = []\n",
    "# ToDo define range\n",
    "#for word in top:\n",
    "#    labels.append(str(iyear)) \n",
    "for label, x, y in zip(top_words_hillary, mds_coordinates_hillary[:,0], mds_coordinates_hillary[:,1]):\n",
    "    plt.annotate(label, (x,y), xycoords = 'data',  fontsize = 18)\n",
    "plt.xlabel('First Dimension')\n",
    "plt.ylabel('Second Dimension')\n",
    "plt.title('Pro-Hillary tweets. Distance between terms')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(mds_coordinates_trump[:,0],mds_coordinates_trump[:,1],\\\n",
    "    facecolors = 'none', edgecolors = 'none')  # plots points in white (invisible)\n",
    "labels = []\n",
    "# ToDo define range\n",
    "#for word in top:\n",
    "#    labels.append(str(iyear)) \n",
    "for label, x, y in zip(top_words_trump, mds_coordinates_trump[:,0], mds_coordinates_trump[:,1]):\n",
    "    plt.annotate(label, (x,y), xycoords = 'data')\n",
    "plt.xlabel('First Dimension')\n",
    "plt.ylabel('Second Dimension')   \n",
    "plt.title('Pro-Trump tweets. Distance between terms')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(mds_coordinates_neutral[:,0],mds_coordinates_neutral[:,1],\\\n",
    "    facecolors = 'none', edgecolors = 'none')  # plots points in white (invisible)\n",
    "labels = []\n",
    "# ToDo define range\n",
    "#for word in top:\n",
    "#    labels.append(str(iyear)) \n",
    "for label, x, y in zip(top_words_neutral, mds_coordinates_neutral[:,0], mds_coordinates_neutral[:,1]):\n",
    "    plt.annotate(label, (x,y), xycoords = 'data')\n",
    "plt.xlabel('First Dimension')\n",
    "plt.ylabel('Second Dimension') \n",
    "plt.title('Neutral tweets. Distance between terms')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_text_split_all = []\n",
    "\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "        { \"place.country_code\":'US' },\n",
    "        { \"clean_text\" : { '$exists':'true' }},\n",
    "        { \"group\" : { '$exists':'true' }\n",
    "    }]}):\n",
    "    if a:\n",
    "        cleaned_text_split_all.append(a[\"clean_text\"])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text_split_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdm_method = CountVectorizer(max_features = 200, binary = True)\n",
    "examine_tweets_tdm = tdm_method.fit(cleaned_text_split_all)\n",
    "\n",
    "top_words_all = examine_tweets_tdm.get_feature_names()\n",
    "\n",
    "sparse_tweets_tdm = tdm_method.fit_transform(cleaned_text_split_all)\n",
    "# convert sparse matrix into regular terms-by-documents matrix\n",
    "tweets_tdm = sparse_tweets_tdm.todense()\n",
    "# define the documents-by-terms matrix \n",
    "tweets_dtm = tweets_tdm.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# classification of words into groups for further analysis\n",
    "# use transpose of the terms-by-document matrix and cluster analysis\n",
    "# try five clusters/groups of words\n",
    "clustering_method = KMeans(n_clusters = 5, random_state = 9999) \n",
    "clustering_solution = clustering_method.fit(tweets_tdm)\n",
    "cluster_membership = clustering_method.predict(tweets_tdm)\n",
    "word_distance_to_center = clustering_method.transform(tweets_tdm)\n",
    "\n",
    "\n",
    "# top words data frame for reporting k-means clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['dist_to_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['dist_to_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['dist_to_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['dist_to_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words_data['dist_to_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "top_words_data_frame = pd.DataFrame(dict([ (k,Series(v)) for k,v in top_words_data.iteritems() ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# top words data frame for reporting k-means clustering results\n",
    "#top_words_data = {'cluster': cluster_membership,\\\n",
    "top_words_data = {'word': top_words_all, 'cluster': cluster_membership,\\\n",
    "    'dist_to_0': word_distance_to_center[0:,0],\\\n",
    "    'dist_to_1': word_distance_to_center[0:,1],\\\n",
    "    'dist_to_2': word_distance_to_center[0:,2],\\\n",
    "    'dist_to_3': word_distance_to_center[0:,3],\\\n",
    "    'dist_to_4': word_distance_to_center[0:,4]}\n",
    "distance_name_list = ['dist_to_0','dist_to_1','dist_to_2','dist_to_3','dist_to_4']    \n",
    "#top_words_data_frame = pd.DataFrame(top_words_data).from_dict(top_words_data, orient='index')\n",
    "for cluster in range(5):\n",
    "    words_in_cluster =\\\n",
    "        top_words_data_frame[top_words_data_frame['cluster'] == cluster] \n",
    "    sorted_data_frame =\\\n",
    "        top_words_data_frame.sort_index(by = distance_name_list[cluster],\\\n",
    "        ascending = True)\n",
    "    print('\\n Top Words in Cluster :',cluster,'------------------------------')\n",
    "    print(sorted_data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we gave up with this approach as we don't think topic groupping is not the best idea in this context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assorted terms by group, font-size relative to frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# end of https://de.dariah.eu/tatom/topic_model_visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text_hillary = []\n",
    "cleaned_text_trump = []\n",
    "cleaned_text_neutral = []\n",
    "\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "        { \"place.country_code\":'US' },\n",
    "        { \"clean_text\" : { '$exists':'true' }},\n",
    "        { \"group\" : { '$exists':'true' }\n",
    "    }]}):\n",
    "    if a and a[\"group\"]==\"hillary\":\n",
    "        cleaned_text_hillary.append(a[\"clean_text\"])\n",
    "    elif a and a[\"group\"]==\"trump\":\n",
    "        cleaned_text_trump.extend(a[\"clean_text\"])\n",
    "    elif a and a[\"group\"]==\"neutral\":\n",
    "        cleaned_text_neutral.extend(a[\"clean_text\"])\n",
    "    else:\n",
    "        pass\n",
    "# take relative word frequencies into account, lower max_font_size\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word cloud with USA flag shape and group-color segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    if (cleaned_text_split_trump).count(word) > (cleaned_text_split_hillary).count(word):\n",
    "        return \"rgb(255,0,0)\"\n",
    "    elif (cleaned_text_split_trump).count(word) < (cleaned_text_split_hillary).count(word):\n",
    "        return \"rgb(0,0,255)\"\n",
    "    else:\n",
    "        return \"rgb(255, 255, 255)\"\n",
    "    \n",
    "        #return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "\n",
    "D_DIR = 'D:\\Master\\FDS\\works\\work2\\wordcloud'\n",
    "usa_mask = np.array(Image.open(path.join(D_DIR, \"eeuumask.jpg\")))\n",
    "usa_mask_color = np.array(Image.open(path.join(D_DIR, \"eeuumaskcolor.jpg\")))\n",
    "\n",
    "wordcloud = WordCloud(mask=usa_mask_color,\n",
    "                      height=800, width=1100,max_font_size=200,\n",
    "                      relative_scaling=.05, background_color=\"white\").generate(\" \".join(cleaned_text_split_trump + cleaned_text_split_hillary))\n",
    "\n",
    "\n",
    "#image_colors = ImageColorGenerator(usa_mask_color)  # trying different approaches with WordCloud\n",
    "\n",
    "plt.imshow(wordcloud.recolor(color_func=color_func, random_state=3))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
