{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the different charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in order to execute in kike's computer... \n",
    "# if missing any submodule from nltk, run in python: nltk.download()\n",
    "\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from textblob import TextBlob\n",
    "# prepare for Python version 3x features and functions\n",
    "from __future__ import division, print_function\n",
    "\n",
    "# import packages for text processing and multivariate analysis\n",
    "import re  # regular expressions\n",
    "import nltk  # draw on the Python natural language toolkit\n",
    "import pandas as pd  # DataFrame structure and operations\n",
    "import numpy as np  # arrays and numerical processing\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt  # 2D plotting\n",
    "\n",
    "# terms-by-documents matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans  # cluster analysis by partitioning\n",
    "from sklearn.decomposition import PCA  # principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steps to use MongoDB:\n",
    "# 1. pip install pymongo\n",
    "# 2. install MongoDB for windows\n",
    "# 3. create folder C:\\data\\db\n",
    "# 4. C:\\Program Files\\MongoDB\\Server\\3.2\\bin>mongoimport.exe /collection:election_tweets D:\\Master\\FDS\\works\\work2\\data\\geotagged_tweets_20160812-0912.jsons\n",
    "# 5. C:\\Program Files\\MongoDB\\Server\\3.2\\bin>mongod.exe\n",
    "\n",
    "\n",
    "client = MongoClient()\n",
    "#client.server_info()  # test purpose\n",
    "db = client.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of tweets over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from math import ceil, log\n",
    "days = {}\n",
    "days[\"neutral\"] = []\n",
    "days[\"trump\"] = []\n",
    "days[\"hillary\"] = []\n",
    "\n",
    "for tweet in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "            { \"place.country_code\":'US' },\n",
    "            { \"clean\" : { '$exists':True }},\n",
    "            { \"group\" : { '$exists':True }}\n",
    "        ]}):\n",
    "    date = datetime.datetime.fromtimestamp(float(unicode(tweet[\"timestamp_ms\"])) / 1e3)\n",
    "    days[tweet[\"group\"]].append(date.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "axt = plt.subplot(311)\n",
    "axt.hist(days[\"trump\"], bins= 8,color=\"red\", align='left')\n",
    "#axt.set_xlim([11,20])\n",
    "axt.set_ylabel('Number of tweets')\n",
    "\n",
    "axh = plt.subplot(312)\n",
    "axh.hist(days[\"hillary\"], bins= 8,color=\"blue\", align='left')\n",
    "#axh.set_xlim([11,20])\n",
    "axh.set_ylabel('Number of tweets')\n",
    "\n",
    "axn = plt.subplot(313)\n",
    "axn.hist(days[\"neutral\"], bins= 8,color=\"green\", align='left')\n",
    "#axn.set_xlim([11,20])\n",
    "axn.set_xlabel('Time')\n",
    "axn.set_ylabel('Number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of subjectivity and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sub = {}\n",
    "sub[\"neutral\"] = []\n",
    "sub[\"trump\"] = []\n",
    "sub[\"hillary\"] = []\n",
    "pol = {}\n",
    "pol[\"neutral\"] = []\n",
    "pol[\"trump\"] = []\n",
    "pol[\"hillary\"] = []\n",
    "for tweet in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "            { \"place.country_code\":'US' },\n",
    "            { \"subjectivity\" : { '$exists':\"true\" }},\n",
    "            { \"group\" : { '$exists':\"true\" }}\n",
    "        ]}):\n",
    "    sub[tweet[\"group\"]].append(tweet['subjectivity'])\n",
    "    pol[tweet[\"group\"]].append(tweet['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "ax1 = plt.subplot(311)\n",
    "ax1.hist(sub[\"trump\"], bins= 10,color=\"red\", align='left')\n",
    "ax1.set_xlim([-0.05,1])\n",
    "ax1.set_ylim([0,400])\n",
    "ax1.grid(True)\n",
    "#ax1.set_xlabel('subjectivity')\n",
    "ax1.set_ylabel('Number of tweets')\n",
    "ax2 = plt.subplot(312)\n",
    "ax2.hist(sub[\"hillary\"], bins= 10,color=\"blue\", align='left')\n",
    "ax2.set_xlim([-0.05,1])\n",
    "ax2.set_ylim([0,600])\n",
    "ax2.grid(True)\n",
    "#ax2.set_xlabel('subjectivity')\n",
    "ax2.set_ylabel('Number of tweets')\n",
    "ax3 = plt.subplot(313)\n",
    "ax3.hist(sub[\"neutral\"], bins= 10,color=\"green\", align='left')\n",
    "ax3.set_xlim([-0.05,1])\n",
    "ax3.set_ylim([0,2000])\n",
    "ax3.set_xlabel('subjectivity')\n",
    "ax3.set_ylabel('Number of tweets')\n",
    "ax3.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "ax4 = plt.subplot(311)\n",
    "ax4.hist(pol[\"trump\"], bins= 10,color=\"red\", align='left')\n",
    "#ax4.set_xlabel('polarity')\n",
    "ax4.set_ylabel('Number of tweets')\n",
    "ax4.set_xlim([-1.05,1.05])\n",
    "ax4.set_ylim([0,300])\n",
    "ax4.grid(True)\n",
    "ax5 = plt.subplot(312)\n",
    "ax5.hist(pol[\"hillary\"], bins= 10,color=\"blue\", align='left')\n",
    "#ax5.set_xlabel('polarity')\n",
    "ax5.set_ylabel('Number of tweets')\n",
    "ax5.set_xlim([-1.05,1.05])\n",
    "ax5.set_ylim([0,300])\n",
    "ax5.grid(True)\n",
    "ax6 = plt.subplot(313)\n",
    "ax6.hist(pol[\"neutral\"], bins= 10,color=\"green\", align='left')\n",
    "ax6.set_xlabel('polarity')\n",
    "ax6.set_ylabel('Number of tweets')\n",
    "ax6.set_xlim([-1.05,1.05])\n",
    "ax6.set_ylim([0,2000])\n",
    "ax6.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud of the pro-trump and pro-hillary groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cleaned_words_split_hillary = []\n",
    "cleaned_words_split_trump = []\n",
    "cleaned_words_split_neutral = []\n",
    "\n",
    "for a in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "        { \"place.country_code\":'US' },\n",
    "        { \"clean\" : { '$exists':'true' }},\n",
    "        { \"group\" : { '$exists':'true' }\n",
    "    }]}):\n",
    "    if a and a[\"group\"]==\"hillary\":\n",
    "        cleaned_words_split_hillary.extend(a[\"clean_text\"].split())\n",
    "    elif a and a[\"group\"]==\"trump\":\n",
    "        cleaned_words_split_trump.extend(a[\"clean_text\"].split())\n",
    "    elif a and a[\"group\"]==\"neutral\":\n",
    "        cleaned_words_split_neutral.extend(a[\"clean_text\"].split())\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Hillary 1st 50 words: \")\n",
    "print(cleaned_words_split_hillary[:50])\n",
    "\n",
    "print(\"Trump 1st 50 words: \")\n",
    "print(cleaned_words_split_trump[:50])\n",
    "\n",
    "print(\"Neutral 1st 50 words: \")\n",
    "print(cleaned_words_split_neutral[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_hillary = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)\n",
    "vectorizer_trump = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)\n",
    "vectorizer_neutral = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_features_hillary = vectorizer_hillary.fit_transform(cleaned_words_split_hillary).toarray()\n",
    "data_features_trump = vectorizer_trump.fit_transform(cleaned_words_split_trump).toarray()\n",
    "data_features_neutral = vectorizer_neutral.fit_transform(cleaned_words_split_neutral).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_features_hillary.shape)\n",
    "print(data_features_trump.shape)\n",
    "print(data_features_neutral.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_hillary = vectorizer_hillary.get_feature_names()\n",
    "vocab_trump = vectorizer_trump.get_feature_names()\n",
    "vocab_neutral = vectorizer_neutral.get_feature_names()\n",
    "print(vocab_hillary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features_hillary, axis=0)\n",
    "vocabulary_count_hillary = []\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab_hillary, dist):\n",
    "    vocabulary_count_hillary.append((count, tag))\n",
    "\n",
    "vocabulary_count_hillary.sort(key = lambda tup: -tup[0])\n",
    "#vocabulary_count.sort(key=lambda tup: tup[1])\n",
    "print(str(vocabulary_count_hillary[0:50]))\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features_trump, axis=0)\n",
    "vocabulary_count_trump = []\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab_trump, dist):\n",
    "    vocabulary_count_trump.append((count, tag))\n",
    "\n",
    "vocabulary_count_trump.sort(key = lambda tup: -tup[0])\n",
    "#vocabulary_count.sort(key=lambda tup: tup[1])\n",
    "print(\"\\n\" )\n",
    "print(str(vocabulary_count_trump[0:50]))\n",
    "\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features_neutral, axis=0)\n",
    "vocabulary_count_neutral = []\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab_neutral, dist):\n",
    "    vocabulary_count_neutral.append((count, tag))\n",
    "\n",
    "vocabulary_count_neutral.sort(key = lambda tup: -tup[0])\n",
    "#vocabulary_count.sort(key=lambda tup: tup[1])\n",
    "print(\"\\n\" )\n",
    "print(str(vocabulary_count_neutral[0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Read the whole text.\n",
    "text = \" \".join(vocab_hillary)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(height=600, width=800).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "# take relative word frequencies into account, lower max_font_size\n",
    "#'''\n",
    "wordcloud = WordCloud(height=800, width=1100,max_font_size=70, relative_scaling=.3,background_color=\"white\").generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the whole text.\n",
    "text = \" \".join(vocab_trump)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(height=600, width=800).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "# take relative word frequencies into account, lower max_font_size\n",
    "#'''\n",
    "wordcloud = WordCloud(height=800, width=1100,max_font_size=70, relative_scaling=.3,background_color=\"white\").generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    if (cleaned_words_split_trump).count(word) > (cleaned_words_split_hillary).count(word):\n",
    "        return \"rgb(255,0,0)\"\n",
    "    elif (cleaned_words_split_trump).count(word) < (cleaned_words_split_hillary).count(word):\n",
    "        return \"rgb(0,0,255)\"\n",
    "    else:\n",
    "        return \"rgb(255, 255, 255)\"\n",
    "    \n",
    "        #return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "\n",
    "D_DIR = 'C:/Users/vincent/Documents/UvA/DataScience/Fundamentals of DS/Week 2'\n",
    "usa_mask = np.array(Image.open(path.join(D_DIR, \"flag.jpg\")))\n",
    "usa_mask_color = np.array(Image.open(path.join(D_DIR, \"flag.jpg\")))\n",
    "\n",
    "wordcloud = WordCloud(mask=usa_mask_color,\n",
    "                      height=800, width=1100,max_font_size=200,\n",
    "                      relative_scaling=.05, background_color=\"white\").generate(\" \".join(cleaned_words_split_trump + cleaned_words_split_hillary))\n",
    "\n",
    "\n",
    "#image_colors = ImageColorGenerator(usa_mask_color)\n",
    "\n",
    "# show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "plt.imshow(wordcloud.recolor(color_func=color_func, random_state=3))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting word frequencies in a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdm_method_hillary = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "tdm_method_trump = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "tdm_method_neutral = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "\n",
    "examine_tdm_hillary = tdm_method_hillary.fit(cleaned_words_split_hillary)\n",
    "examine_tdm_trump = tdm_method_trump.fit(cleaned_words_split_trump)\n",
    "examine_tdm_neutral = tdm_method_neutral.fit(cleaned_words_split_neutral)\n",
    "\n",
    "top_words_hillary = examine_tdm_hillary.get_feature_names()\n",
    "top_words_trump = examine_tdm_trump.get_feature_names()\n",
    "top_words_neutral = examine_tdm_neutral.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances \n",
    "from sklearn.metrics.pairwise import linear_kernel as cosine_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances as manhattan_distances\n",
    "\n",
    "from sklearn import manifold  # multidimensional scaling\n",
    "# get clean printing of the top words \n",
    "#print(map(lambda t: t.encode('ascii'), top_words))  # print sans unicode\n",
    "\n",
    "# extract the terms-by-documents matrix \n",
    "# in scipy compressed sparse column format\n",
    "sparse_tdm_hillary = tdm_method_hillary.fit_transform(cleaned_words_split_hillary)\n",
    "sparse_tdm_trump = tdm_method_trump.fit_transform(cleaned_words_split_trump)\n",
    "sparse_tdm_neutral = tdm_method_neutral.fit_transform(cleaned_words_split_neutral)\n",
    "# convert sparse matrix into regular terms-by-documents matrix\n",
    "tweet_tdm_hillary = sparse_tdm_hillary.todense()\n",
    "tweet_tdm_trump = sparse_tdm_trump.todense()\n",
    "tweet_tdm_neutral = sparse_tdm_neutral.todense()\n",
    "# define the documents-by-terms matrix \n",
    "tweet_dtm_hillary = tweet_tdm_hillary.transpose()\n",
    "tweet_dtm_trump = tweet_tdm_trump.transpose()\n",
    "tweet_dtm_neutral = tweet_tdm_neutral.transpose()\n",
    "\n",
    "# movies_distance_matrix = euclidean_distances(tweet_dtm)\n",
    "# movies_distance_matrix = manhattan_distances(tweet_dtm)\n",
    "# movies_distance_matrix = cosine_distances(tweet_dtm)\n",
    "# for some reason, manhattan_distances yields all-zeros\n",
    "tweet_distance_matrix_hillary = manhattan_distances(tweet_dtm_hillary)\n",
    "tweet_distance_matrix_trump = manhattan_distances(tweet_dtm_trump)\n",
    "tweet_distance_matrix_neutral = manhattan_distances(tweet_dtm_neutral)\n",
    "\n",
    "# multidimensional scaling\n",
    "mds_method_hillary = manifold.MDS(n_components = 2, random_state = 9999,\\\n",
    "    dissimilarity = 'precomputed')\n",
    "mds_fit_hillary = mds_method_hillary.fit(tweet_distance_matrix_hillary)  \n",
    "mds_coordinates_hillary = mds_method_hillary.fit_transform(tweet_distance_matrix_hillary) \n",
    "\n",
    "mds_method_trump = manifold.MDS(n_components = 2, random_state = 9999,\\\n",
    "    dissimilarity = 'precomputed')\n",
    "mds_fit_trump = mds_method_trump.fit(tweet_distance_matrix_trump)  \n",
    "mds_coordinates_trump = mds_method_trump.fit_transform(tweet_distance_matrix_trump) \n",
    "\n",
    "mds_method_neutral = manifold.MDS(n_components = 2, random_state = 9999,\\\n",
    "    dissimilarity = 'precomputed')\n",
    "mds_fit_neutral = mds_method_neutral.fit(tweet_distance_matrix_neutral)  \n",
    "mds_coordinates_neutral = mds_method_neutral.fit_transform(tweet_distance_matrix_neutral) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(mds_coordinates_hillary[:,0],mds_coordinates_hillary[:,1],\\\n",
    "    facecolors = 'none', edgecolors = 'none')  # plots points in white (invisible)\n",
    "labels = []\n",
    "# ToDo define range\n",
    "#for word in top:\n",
    "#    labels.append(str(iyear)) \n",
    "for label, x, y in zip(top_words_hillary, mds_coordinates_hillary[:,0], mds_coordinates_hillary[:,1]):\n",
    "    plt.annotate(label, (x,y), xycoords = 'data',  fontsize = 18)\n",
    "plt.xlabel('First Dimension')\n",
    "plt.ylabel('Second Dimension')\n",
    "plt.title('Pro-Hillary tweets. Distance between terms')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(mds_coordinates_trump[:,0],mds_coordinates_trump[:,1],\\\n",
    "    facecolors = 'none', edgecolors = 'none')  # plots points in white (invisible)\n",
    "labels = []\n",
    "# ToDo define range\n",
    "#for word in top:\n",
    "#    labels.append(str(iyear)) \n",
    "for label, x, y in zip(top_words_trump, mds_coordinates_trump[:,0], mds_coordinates_trump[:,1]):\n",
    "    plt.annotate(label, (x,y), xycoords = 'data')\n",
    "plt.xlabel('First Dimension')\n",
    "plt.ylabel('Second Dimension')   \n",
    "plt.title('Pro-Trump tweets. Distance between terms')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(mds_coordinates_neutral[:,0],mds_coordinates_neutral[:,1],\\\n",
    "    facecolors = 'none', edgecolors = 'none')  # plots points in white (invisible)\n",
    "labels = []\n",
    "# ToDo define range\n",
    "#for word in top:\n",
    "#    labels.append(str(iyear)) \n",
    "for label, x, y in zip(top_words_neutral, mds_coordinates_neutral[:,0], mds_coordinates_neutral[:,1]):\n",
    "    plt.annotate(label, (x,y), xycoords = 'data')\n",
    "plt.xlabel('First Dimension')\n",
    "plt.ylabel('Second Dimension') \n",
    "plt.title('Neutral tweets. Distance between terms')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the trump supporters vs the hillary supporters on the map of the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pygeocoder import Geocoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = {}\n",
    "data[\"trump\"] = []\n",
    "data[\"hillary\"] = []\n",
    "for tweet in db.get_collection('election_tweets').find(\n",
    "    {'$and':[\n",
    "        { \"place.country_code\":'US' },\n",
    "        { \"clean_text\" : { '$exists':'true' }},\n",
    "        { \"group\" : { '$exists':'true' }\n",
    "    }]}):\n",
    "    if tweet and tweet[\"group\"]==\"trump\":\n",
    "        data[\"trump\"].append(tweet['place']['bounding_box']['coordinates'][0][0])\n",
    "    elif tweet and tweet[\"group\"]==\"hillary\":\n",
    "        data[\"hillary\"].append(tweet['place']['bounding_box']['coordinates'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create two lists for the loop results to be placed\n",
    "lat_trump = []\n",
    "lon_trump = []\n",
    "\n",
    "# For each row in a varible,\n",
    "for row in data[\"trump\"]:\n",
    "    # Try to,\n",
    "    try:\n",
    "        # Split the row by comma, convert to float, and append\n",
    "        # everything before the comma to lat\n",
    "        lat_trump.append(float(row[0]))\n",
    "        # Split the row by comma, convert to float, and append\n",
    "        # everything after the comma to lon\n",
    "        lon_trump.append(float(row[1]))\n",
    "    # But if you get an error\n",
    "    except:\n",
    "        # append a missing value to lat\n",
    "        lat_trump.append(np.NaN)\n",
    "        # append a missing value to lon\n",
    "        lon_trump.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two lists for the loop results to be placed\n",
    "lat_hillary = []\n",
    "lon_hillary = []\n",
    "\n",
    "# For each row in a varible,\n",
    "for row in data[\"hillary\"]:\n",
    "    # Try to,\n",
    "    try:\n",
    "        # Split the row by comma, convert to float, and append\n",
    "        # everything before the comma to lat\n",
    "        lat_hillary.append(float(row[0]))\n",
    "        # Split the row by comma, convert to float, and append\n",
    "        # everything after the comma to lon\n",
    "        lon_hillary.append(float(row[1]))\n",
    "    # But if you get an error\n",
    "    except:\n",
    "        # append a missing value to lat\n",
    "        lat_hillary.append(np.NaN)\n",
    "        # append a missing value to lon\n",
    "        lon_hillary.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "results_trump = []\n",
    "for k in range(len(lon_trump)):\n",
    "    try:\n",
    "        temp = Geocoder(\"AIzaSyAUamlGx26SvTjRu5trfMu61PQrInpbes4\").reverse_geocode(lon_trump[k],lat_trump[k],)\n",
    "        #test = temp.administrative_area_level_1\n",
    "        results_trump.append(temp.administrative_area_level_1.encode('ascii'))        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "results_hillary = []\n",
    "for k in range(len(lon_hillary)):\n",
    "    try:\n",
    "        temp = Geocoder(\"AIzaSyAUamlGx26SvTjRu5trfMu61PQrInpbes4\").reverse_geocode(lon_hillary[k],lat_hillary[k],)\n",
    "        #test = temp.administrative_area_level_1\n",
    "        results_hillary.append(temp.administrative_area_level_1.encode('ascii'))        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotdata = Counter(results_trump)\n",
    "print(plotdata)\n",
    "plt.figure()\n",
    "plt.bar(range(len(plotdata)), plotdata.values(), align='center',color=\"white\")\n",
    "plt.grid()\n",
    "plt.xticks(range(len(plotdata)), plotdata.keys())\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(str(sum(plotdata.viewvalues())) + \" from \" + str(len(data)) + \" represented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotdata = Counter(results_hillary)\n",
    "print(plotdata)\n",
    "plt.figure()\n",
    "plt.bar(range(len(plotdata)), plotdata.values(), align='center',color=\"white\")\n",
    "plt.grid()\n",
    "plt.xticks(range(len(plotdata)), plotdata.keys())\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(str(sum(plotdata.viewvalues())) + \" from \" + str(len(data)) + \" represented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barchart of topics vs segmented groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first approach using NMF to get a document-topic matrix\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtm = vectorizer.fit_transform(tweets).toarray()\n",
    "# vocab = np.array(vectorizer.get_feature_names())\n",
    "from sklearn import decomposition\n",
    "num_topics = 20\n",
    "num_top_words = 20\n",
    "clf_trump = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "clf_hillary = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "#clf_neutral = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "# this next step may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdm_method_hillary = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "tdm_method_trump = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "#tdm_method_neutral = CountVectorizer(max_features = 100, binary = True) # term document matrix\n",
    "\n",
    "examine_tdm_hillary = tdm_method_hillary.fit_transform(cleaned_words_split_hillary).toarray()\n",
    "examine_tdm_trump = tdm_method_trump.fit_transform(cleaned_words_split_trump).toarray()\n",
    "#examine_tdm_neutral = tdm_method_neutral.fit_transform(cleaned_words_split_neutral).toarray()\n",
    "\n",
    "doctopic_trump = clf_trump.fit_transform(examine_tdm_trump)\n",
    "doctopic_hillary = clf_hillary.fit_transform(examine_tdm_hillary)\n",
    "#doctopic_neutral = clf_neutral.fit_transform(examine_tdm_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print words associated with topics\n",
    "topic_words_trump = []\n",
    "for topic in clf_trump.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words_trump.append([vocab_trump[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print words associated with topics\n",
    "topic_words_hillary = []\n",
    "for topic in clf_hillary.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words_hillary.append([vocab_hillary[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scalling\n",
    "doctopic_trump = doctopic_trump / np.sum(doctopic_trump, axis=1, keepdims=True)\n",
    "# scalling\n",
    "doctopic_hillary = doctopic_hillary / np.sum(doctopic_hillary, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stacked barchart\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, K = doctopic_trump.shape  # N documents, K topics\n",
    "\n",
    "ind = np.arange(N)  # the x-axis locations for the novels\n",
    "\n",
    "width = 0.5  # the width of the bars\n",
    "\n",
    "plots = []\n",
    "\n",
    "height_cumulative = np.zeros(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    color = plt.cm.coolwarm(k/K, 1)\n",
    "    if k == 0:\n",
    "        p = plt.bar(ind, doctopic_trump[:, k], width, color=color)\n",
    "    else:\n",
    "        p = plt.bar(ind, doctopic_trump[:, k], width, bottom=height_cumulative, color=color)\n",
    "    height_cumulative += doctopic_trump[:, k]\n",
    "    plots.append(p)\n",
    " \n",
    "\n",
    "plt.ylim((0, 1))  # proportions sum to 1, so the height of the stacked bars is 1\n",
    "\n",
    "\n",
    "plt.ylabel('Topics')\n",
    "\n",
    "\n",
    "plt.title('Topics in novels')\n",
    "\n",
    "# should be groups\n",
    "plt.xticks(ind+width/2, groups)\n",
    "\n",
    "plt.yticks(np.arange(0, 1, 10))\n",
    "\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "\n",
    "# see http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend for details\n",
    "# on making a legend in matplotlib\n",
    "plt.legend([p[0] for p in plots], topic_labels)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
